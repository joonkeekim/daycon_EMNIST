{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"daycon_EMNIST.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMF1DZB4dTn2Hzg/fnXwd2N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LcXxpTyE-PFe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1598793865612,"user_tz":-540,"elapsed":28082,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}},"outputId":"47f32e35-a4ef-4717-fc9c-e75228e399a0"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iDadLTbb-p1u","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598794604397,"user_tz":-540,"elapsed":2929,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7KPAqtb_rTz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598794610001,"user_tz":-540,"elapsed":4160,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["train = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/dacon/train.csv',index_col=0)\n","test = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/dacon/test.csv',index_col=0)\n","submission = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/dacon/submission.csv',index_col=0)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAcgazgzACGF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598794610002,"user_tz":-540,"elapsed":3527,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}},"outputId":"7d63a254-1297-4501-a03d-9d3ad54d1f04"},"source":["train.shape"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2048, 786)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"ZOdx8Xv0AFHM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":284},"executionInfo":{"status":"ok","timestamp":1598794610003,"user_tz":-540,"elapsed":3195,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}},"outputId":"b903257a-e08d-499d-9b04-a79e25dabe27"},"source":["train.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>digit</th>\n","      <th>letter</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>...</th>\n","      <th>744</th>\n","      <th>745</th>\n","      <th>746</th>\n","      <th>747</th>\n","      <th>748</th>\n","      <th>749</th>\n","      <th>750</th>\n","      <th>751</th>\n","      <th>752</th>\n","      <th>753</th>\n","      <th>754</th>\n","      <th>755</th>\n","      <th>756</th>\n","      <th>757</th>\n","      <th>758</th>\n","      <th>759</th>\n","      <th>760</th>\n","      <th>761</th>\n","      <th>762</th>\n","      <th>763</th>\n","      <th>764</th>\n","      <th>765</th>\n","      <th>766</th>\n","      <th>767</th>\n","      <th>768</th>\n","      <th>769</th>\n","      <th>770</th>\n","      <th>771</th>\n","      <th>772</th>\n","      <th>773</th>\n","      <th>774</th>\n","      <th>775</th>\n","      <th>776</th>\n","      <th>777</th>\n","      <th>778</th>\n","      <th>779</th>\n","      <th>780</th>\n","      <th>781</th>\n","      <th>782</th>\n","      <th>783</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>5</td>\n","      <td>L</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>B</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>L</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9</td>\n","      <td>D</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>A</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 786 columns</p>\n","</div>"],"text/plain":["    digit letter  0  1  2  3  4  5  ...  776  777  778  779  780  781  782  783\n","id                                  ...                                        \n","1       5      L  1  1  1  4  3  0  ...    0    1    2    4    4    4    3    4\n","2       0      B  0  4  0  0  4  1  ...    0    1    4    1    4    2    1    2\n","3       4      L  1  1  2  2  1  1  ...    3    0    2    0    3    0    2    2\n","4       9      D  1  2  0  2  0  4  ...    2    0    1    4    0    0    1    1\n","5       6      A  3  0  2  4  0  3  ...    3    2    1    3    4    3    1    2\n","\n","[5 rows x 786 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"4bq356jWGBKl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598794610003,"user_tz":-540,"elapsed":2720,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}},"outputId":"80842e61-a3dd-4273-9bc0-a96feeafe937"},"source":["test.shape"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20480, 785)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"4cfqudcMGDAm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"status":"ok","timestamp":1598794610412,"user_tz":-540,"elapsed":2634,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}},"outputId":"911a09a9-47d7-4af5-a8d1-37d861ee7b44"},"source":["import random\n","rndidx = random.randint(0,2048)\n","\n","img = np.array(train.iloc[rndidx,2:]).reshape(28,28).astype(np.float32)\n","plt.title(train.iloc[rndidx,1])\n","plt.imshow(img)\n","plt.show()\n","print(train.iloc[rndidx,0])"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"shaOyXKOGWM2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598794610414,"user_tz":-540,"elapsed":1238,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}},"outputId":"003376e2-c2e3-4255-d621-149658a45b67"},"source":["from keras.utils.np_utils import to_categorical\n","x_train = np.array(train.iloc[:,2:]).reshape(-1,28,28,1).astype(np.float32)\n","y_train = to_categorical(train['digit'].values)\n","\n","x_train = x_train/255.\n","\n","x_train.shape, y_train.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((2048, 28, 28, 1), (2048, 10))"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"_2eqhJhLH6kT","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598794610694,"user_tz":-540,"elapsed":803,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.1)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"rnSttRDoKrQR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598794613474,"user_tz":-540,"elapsed":661,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["datagen = keras.preprocessing.image.ImageDataGenerator(\n","        rotation_range=10,  \n","        zoom_range = 0.10,  \n","        width_shift_range=0.1, \n","        height_shift_range=0.1)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPnDJZ91lX74","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598794771521,"user_tz":-540,"elapsed":666,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["from keras.layers import Dense, Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"EEdQiIQnmDYU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598795038967,"user_tz":-540,"elapsed":839,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["from keras.models import Model\n","from keras.layers import Dense, Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D, BatchNormalization, Activation, Dropout\n","from keras import backend as K\n","\n","def conv_block(units, dropout=0.2, activation='relu', block=1, layer=1):\n","\n","    def layer_wrapper(inp):\n","        x = Conv2D(units, (3, 3), padding='same', name='block{}_conv{}'.format(block, layer))(inp)\n","        x = BatchNormalization(name='block{}_bn{}'.format(block, layer))(x)\n","        x = Activation(activation, name='block{}_act{}'.format(block, layer))(x)\n","        x = Dropout(dropout, name='block{}_dropout{}'.format(block, layer))(x)\n","        return x\n","\n","    return layer_wrapper\n","\n","def dense_block(units, dropout=0.2, activation='relu', name='fc1'):\n","\n","    def layer_wrapper(inp):\n","        x = Dense(units, name=name)(inp)\n","        x = BatchNormalization(name='{}_bn'.format(name))(x)\n","        x = Activation(activation, name='{}_act'.format(name))(x)\n","        x = Dropout(dropout, name='{}_dropout'.format(name))(x)\n","        return x\n","\n","    return layer_wrapper\n","        \n","\n","def VGG16_BN(input_tensor=None, input_shape=None, classes=1000, conv_dropout=0.1, dropout=0.3, activation='relu'):\n","    \"\"\"Instantiates the VGG16 architecture with Batch Normalization\n","    # Arguments\n","        input_tensor: Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model.\n","        input_shape: shape tuple\n","        classes: optional number of classes to classify images\n","    # Returns\n","        A Keras model instance.\n","    \"\"\"\n","    img_input = Input(shape=input_shape) if input_tensor is None else (\n","        Input(tensor=input_tensor, shape=input_shape) if not K.is_keras_tensor(input_tensor) else input_tensor\n","    )\n","\n","    # Block 1\n","    x = conv_block(32, dropout=conv_dropout, activation=activation, block=1, layer=1)(img_input)\n","    x = conv_block(32, dropout=conv_dropout, activation=activation, block=1, layer=2)(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n","\n","    # Block 2\n","    x = conv_block(64, dropout=conv_dropout, activation=activation, block=2, layer=1)(x)\n","    x = conv_block(64, dropout=conv_dropout, activation=activation, block=2, layer=2)(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n","\n","    # Block 3\n","    x = conv_block(128, dropout=conv_dropout, activation=activation, block=3, layer=1)(x)\n","    x = conv_block(128, dropout=conv_dropout, activation=activation, block=3, layer=2)(x)\n","    x = conv_block(128, dropout=conv_dropout, activation=activation, block=3, layer=3)(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n","\n","    # Block 4\n","    x = conv_block(256, dropout=conv_dropout, activation=activation, block=4, layer=1)(x)\n","    x = conv_block(256, dropout=conv_dropout, activation=activation, block=4, layer=2)(x)\n","    x = conv_block(256, dropout=conv_dropout, activation=activation, block=4, layer=3)(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n","\n","    # Block 5\n","    x = conv_block(256, dropout=conv_dropout, activation=activation, block=5, layer=1)(x)\n","    x = conv_block(256, dropout=conv_dropout, activation=activation, block=5, layer=2)(x)\n","    x = conv_block(256, dropout=conv_dropout, activation=activation, block=5, layer=3)(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n","\n","    # Flatten\n","    x = GlobalAveragePooling2D()(x)\n","\n","    # FC Layers\n","    x = dense_block(512, dropout=dropout, activation=activation, name='fc1')(x)\n","    x = dense_block(512, dropout=dropout, activation=activation, name='fc2')(x)\n","    \n","    # Classification block    \n","    x = Dense(classes, activation='softmax', name='predictions')(x)\n","\n","    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n","    inputs = get_source_inputs(input_tensor) if input_tensor is not None else img_input\n","\n","    # Create model.\n","    return Model(inputs, x, name='vgg16_bn')"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fc9jg6KnPCb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598795198683,"user_tz":-540,"elapsed":973,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["#위에거를 틀만가져오자.\n","from keras.models import Model\n","from keras.layers import Dense, Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D, BatchNormalization, Activation, Dropout\n","from keras import backend as K\n","\n","def conv_block(units, dropout=0.2, activation='relu', block=1, layer=1):\n","\n","    def layer_wrapper(inp):\n","        x = Conv2D(units, (3, 3), padding='same', name='block{}_conv{}'.format(block, layer))(inp)\n","        x = BatchNormalization(name='block{}_bn{}'.format(block, layer))(x)\n","        x = Activation(activation, name='block{}_act{}'.format(block, layer))(x)\n","        x = Dropout(dropout, name='block{}_dropout{}'.format(block, layer))(x)\n","        return x\n","\n","    return layer_wrapper\n","\n","def dense_block(units, dropout=0.2, activation='relu', name='fc1'):\n","\n","    def layer_wrapper(inp):\n","        x = Dense(units, name=name)(inp)\n","        x = BatchNormalization(name='{}_bn'.format(name))(x)\n","        x = Activation(activation, name='{}_act'.format(name))(x)\n","        x = Dropout(dropout, name='{}_dropout'.format(name))(x)\n","        return x\n","\n","    return layer_wrapper\n","\n","def VGG16_BN(input_tensor=None, input_shape=None, classes=1000, conv_dropout=0.1, dropout=0.3, activation='relu'):\n","    \"\"\"Instantiates the VGG16 architecture with Batch Normalization\n","    # Arguments\n","        input_tensor: Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model.\n","        input_shape: shape tuple\n","        classes: optional number of classes to classify images\n","    # Returns\n","        A Keras model instance.\n","    \"\"\"\n","    img_input = Input(shape=input_shape) if input_tensor is None else (\n","        Input(tensor=input_tensor, shape=input_shape) if not K.is_keras_tensor(input_tensor) else input_tensor\n","    )\n","\n","    # Block 1\n","    x = conv_block(32, dropout=conv_dropout, activation=activation, block=1, layer=1)(img_input)\n","    x = conv_block(32, dropout=conv_dropout, activation=activation, block=1, layer=2)(x)\n","    x = MaxPooling2D((2, 2), name='block1_pool')(x)\n","\n","    # Block 2\n","    x = conv_block(64, dropout=conv_dropout, activation=activation, block=2, layer=1)(x)\n","    x = conv_block(64, dropout=conv_dropout, activation=activation, block=2, layer=2)(x)\n","    x = MaxPooling2D((2, 2), name='block2_pool')(x)\n","\n","    # Block 3\n","    x = conv_block(128, dropout=conv_dropout, activation=activation, block=3, layer=1)(x)\n","    x = conv_block(128, dropout=conv_dropout, activation=activation, block=3, layer=2)(x)\n","    x = conv_block(128, dropout=conv_dropout, activation=activation, block=3, layer=3)(x)\n","    x = MaxPooling2D((2, 2), name='block3_pool')(x)\n","\n","    # Block 4\n","    x = conv_block(256, dropout=conv_dropout, activation=activation, block=4, layer=1)(x)\n","    x = conv_block(256, dropout=conv_dropout, activation=activation, block=4, layer=2)(x)\n","    x = conv_block(256, dropout=conv_dropout, activation=activation, block=4, layer=3)(x)\n","    x = MaxPooling2D((2, 2), name='block4_pool')(x)\n","\n","    # Block 5\n","    # x = conv_block(256, dropout=conv_dropout, activation=activation, block=5, layer=1)(x)\n","    # x = conv_block(256, dropout=conv_dropout, activation=activation, block=5, layer=2)(x)\n","    # x = conv_block(256, dropout=conv_dropout, activation=activation, block=5, layer=3)(x)\n","    # x = MaxPooling2D((2, 2), name='block5_pool')(x)\n","\n","    # Flatten\n","    x = GlobalAveragePooling2D()(x)\n","\n","    # FC Layers\n","    x = dense_block(512, dropout=dropout, activation=activation, name='fc1')(x)\n","    x = dense_block(512, dropout=dropout, activation=activation, name='fc2')(x)\n","    \n","    # Classification block    \n","    x = Dense(classes, activation='softmax', name='predictions')(x)\n","\n","    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n","    inputs = get_source_inputs(input_tensor) if input_tensor is not None else img_input\n","\n","    # Create model.\n","    return Model(inputs, x, name='vgg16_bn')"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"m2knn4ikmI0Y","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598795199551,"user_tz":-540,"elapsed":1535,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["model = VGG16_BN(input_shape=(28,28,1),classes = 10)\n","\n","model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"wiaK3sGyXxwK","colab_type":"code","colab":{}},"source":["\n","\n","\n","\n","\n","\n","# from tensorflow.keras import activations\n","# model = keras.models.Sequential()\n","\n","# model.add(keras.layers.Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1),padding='same'))\n","# model.add(keras.layers.Conv2D(32, kernel_size = 3,use_bias=False,padding='same'))\n","# model.add(keras.layers.BatchNormalization())\n","# model.add(keras.layers.Activation(activations.relu))\n","\n","# model.add(keras.layers.Conv2D(64, kernel_size = 3, activation='relu',padding='same'))\n","# model.add(keras.layers.Conv2D(64, kernel_size = 3,use_bias=False,padding='same'))\n","# model.add(keras.layers.BatchNormalization())\n","# model.add(keras.layers.Activation(activations.relu))\n","# model.add(keras.layers.MaxPooling2D((2,2)))\n","\n","\n","# model.add(keras.layers.Conv2D(128, kernel_size = 3, activation='relu',padding='same'))\n","# model.add(keras.layers.Conv2D(128, kernel_size = 3,use_bias=False,padding='same'))\n","# model.add(keras.layers.BatchNormalization())\n","# model.add(keras.layers.Activation(activations.relu))\n","\n","\n","# model.add(keras.layers.Conv2D(256, kernel_size = 3, activation='relu',padding='same'))\n","# model.add(keras.layers.Conv2D(256, kernel_size = 3,use_bias=False,padding='same'))\n","# model.add(keras.layers.BatchNormalization())\n","# model.add(keras.layers.Activation(activations.relu))\n","# model.add(keras.layers.MaxPooling2D((2,2)))\n","\n","# model.add(keras.layers.Conv2D(256, kernel_size = 3, activation='relu',padding='same'))\n","# model.add(keras.layers.Flatten())\n","# model.add(keras.layers.Dense(10, activation='softmax'))\n","\n","# # COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST\n","# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","\n","# model = keras.models.Sequential()\n","# model.add(keras.layers.Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n","# model.add(keras.layers.Conv2D(32, kernel_size = 3, activation='relu'))\n","# model.add(keras.layers.Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n","# model.add(keras.layers.Dropout(0.4))\n","\n","# model.add(keras.layers.Conv2D(64, kernel_size = 3, activation='relu'))\n","# model.add(keras.layers.Conv2D(64, kernel_size = 3, activation='relu'))\n","# model.add(keras.layers.Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n","# model.add(keras.layers.Dropout(0.4))\n","\n","# model.add(keras.layers.Conv2D(128, kernel_size = 4, activation='relu'))\n","# model.add(keras.layers.Flatten())\n","# model.add(keras.layers.Dropout(0.4))\n","# model.add(keras.layers.Dense(10, activation='softmax'))\n","\n","# # COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST\n","# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OVsX8WBN6hK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598795299578,"user_tz":-540,"elapsed":96318,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}},"outputId":"a5d46513-9815-47ea-89fe-2a348630e431"},"source":["annealer = keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n","\n","model.compile(optimizer = keras.optimizers.Adam(), loss = 'categorical_crossentropy',metrics=['accuracy'])\n","\n","history = model.fit(\n","    datagen.flow(x_train, y_train, batch_size=32),\n","    epochs = 50,\n","    steps_per_epoch = x_train.shape[0]//32,\n","    validation_data = (x_val, y_val),\n","    callbacks = [annealer],\n","    verbose=1\n",")\n"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","57/57 [==============================] - 3s 46ms/step - loss: 2.5101 - accuracy: 0.1590 - val_loss: 2.3665 - val_accuracy: 0.0976\n","Epoch 2/50\n","57/57 [==============================] - 2s 29ms/step - loss: 2.2767 - accuracy: 0.2137 - val_loss: 2.7558 - val_accuracy: 0.0976\n","Epoch 3/50\n","57/57 [==============================] - 2s 29ms/step - loss: 2.1048 - accuracy: 0.2711 - val_loss: 3.2750 - val_accuracy: 0.0976\n","Epoch 4/50\n","57/57 [==============================] - 2s 30ms/step - loss: 1.8551 - accuracy: 0.3617 - val_loss: 4.2979 - val_accuracy: 0.0976\n","Epoch 5/50\n","57/57 [==============================] - 2s 29ms/step - loss: 1.6126 - accuracy: 0.4495 - val_loss: 5.7395 - val_accuracy: 0.0976\n","Epoch 6/50\n","57/57 [==============================] - 2s 30ms/step - loss: 1.5384 - accuracy: 0.4660 - val_loss: 6.0542 - val_accuracy: 0.0976\n","Epoch 7/50\n","57/57 [==============================] - 2s 29ms/step - loss: 1.3387 - accuracy: 0.5522 - val_loss: 5.7049 - val_accuracy: 0.0976\n","Epoch 8/50\n","57/57 [==============================] - 2s 29ms/step - loss: 1.2147 - accuracy: 0.5831 - val_loss: 5.6781 - val_accuracy: 0.0976\n","Epoch 9/50\n","57/57 [==============================] - 2s 29ms/step - loss: 1.1315 - accuracy: 0.6229 - val_loss: 3.5176 - val_accuracy: 0.1268\n","Epoch 10/50\n","57/57 [==============================] - 2s 30ms/step - loss: 1.0606 - accuracy: 0.6449 - val_loss: 2.4224 - val_accuracy: 0.3756\n","Epoch 11/50\n","57/57 [==============================] - 2s 30ms/step - loss: 1.0140 - accuracy: 0.6676 - val_loss: 1.2012 - val_accuracy: 0.6488\n","Epoch 12/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.9036 - accuracy: 0.7051 - val_loss: 0.8965 - val_accuracy: 0.7171\n","Epoch 13/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.9293 - accuracy: 0.6808 - val_loss: 1.1414 - val_accuracy: 0.6439\n","Epoch 14/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.8111 - accuracy: 0.7267 - val_loss: 0.8057 - val_accuracy: 0.7220\n","Epoch 15/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.7853 - accuracy: 0.7272 - val_loss: 0.7992 - val_accuracy: 0.7122\n","Epoch 16/50\n","57/57 [==============================] - 2s 30ms/step - loss: 0.7300 - accuracy: 0.7565 - val_loss: 0.7810 - val_accuracy: 0.7561\n","Epoch 17/50\n","57/57 [==============================] - 2s 30ms/step - loss: 0.7231 - accuracy: 0.7648 - val_loss: 0.7165 - val_accuracy: 0.7463\n","Epoch 18/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.7062 - accuracy: 0.7626 - val_loss: 0.7466 - val_accuracy: 0.7610\n","Epoch 19/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.6969 - accuracy: 0.7670 - val_loss: 0.6850 - val_accuracy: 0.7805\n","Epoch 20/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.6543 - accuracy: 0.7808 - val_loss: 0.6207 - val_accuracy: 0.8098\n","Epoch 21/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.6287 - accuracy: 0.7863 - val_loss: 0.6898 - val_accuracy: 0.7902\n","Epoch 22/50\n","57/57 [==============================] - 2s 30ms/step - loss: 0.5925 - accuracy: 0.7968 - val_loss: 0.5146 - val_accuracy: 0.8341\n","Epoch 23/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.5643 - accuracy: 0.8001 - val_loss: 0.6420 - val_accuracy: 0.7951\n","Epoch 24/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.5571 - accuracy: 0.8040 - val_loss: 0.5316 - val_accuracy: 0.8195\n","Epoch 25/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.5517 - accuracy: 0.8128 - val_loss: 0.5667 - val_accuracy: 0.7805\n","Epoch 26/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.5174 - accuracy: 0.8172 - val_loss: 0.5908 - val_accuracy: 0.7902\n","Epoch 27/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.5211 - accuracy: 0.8310 - val_loss: 0.5406 - val_accuracy: 0.8098\n","Epoch 28/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.4919 - accuracy: 0.8327 - val_loss: 0.5392 - val_accuracy: 0.8244\n","Epoch 29/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.4476 - accuracy: 0.8437 - val_loss: 0.4938 - val_accuracy: 0.8293\n","Epoch 30/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.4733 - accuracy: 0.8355 - val_loss: 0.5337 - val_accuracy: 0.8195\n","Epoch 31/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.4003 - accuracy: 0.8631 - val_loss: 0.5986 - val_accuracy: 0.8146\n","Epoch 32/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.4677 - accuracy: 0.8470 - val_loss: 0.5146 - val_accuracy: 0.8146\n","Epoch 33/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.3965 - accuracy: 0.8613 - val_loss: 0.4417 - val_accuracy: 0.8683\n","Epoch 34/50\n","57/57 [==============================] - 2s 30ms/step - loss: 0.4120 - accuracy: 0.8614 - val_loss: 0.4689 - val_accuracy: 0.8390\n","Epoch 35/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.4286 - accuracy: 0.8564 - val_loss: 0.5266 - val_accuracy: 0.8439\n","Epoch 36/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.4005 - accuracy: 0.8631 - val_loss: 0.4579 - val_accuracy: 0.8634\n","Epoch 37/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.4085 - accuracy: 0.8668 - val_loss: 0.5411 - val_accuracy: 0.8098\n","Epoch 38/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.3577 - accuracy: 0.8680 - val_loss: 0.5352 - val_accuracy: 0.8537\n","Epoch 39/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.3411 - accuracy: 0.8785 - val_loss: 0.4327 - val_accuracy: 0.8341\n","Epoch 40/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.3215 - accuracy: 0.8934 - val_loss: 0.4605 - val_accuracy: 0.8537\n","Epoch 41/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.3216 - accuracy: 0.8918 - val_loss: 0.5224 - val_accuracy: 0.8293\n","Epoch 42/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.3176 - accuracy: 0.8918 - val_loss: 0.5570 - val_accuracy: 0.8146\n","Epoch 43/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.3032 - accuracy: 0.8945 - val_loss: 0.5497 - val_accuracy: 0.8049\n","Epoch 44/50\n","57/57 [==============================] - 2s 28ms/step - loss: 0.3362 - accuracy: 0.8813 - val_loss: 0.4614 - val_accuracy: 0.8439\n","Epoch 45/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.3122 - accuracy: 0.8907 - val_loss: 0.4902 - val_accuracy: 0.8341\n","Epoch 46/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.2914 - accuracy: 0.8990 - val_loss: 0.4653 - val_accuracy: 0.8390\n","Epoch 47/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.2443 - accuracy: 0.9117 - val_loss: 0.5168 - val_accuracy: 0.8244\n","Epoch 48/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.2585 - accuracy: 0.9061 - val_loss: 0.5413 - val_accuracy: 0.8244\n","Epoch 49/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.2687 - accuracy: 0.9023 - val_loss: 0.5099 - val_accuracy: 0.8390\n","Epoch 50/50\n","57/57 [==============================] - 2s 29ms/step - loss: 0.2740 - accuracy: 0.9056 - val_loss: 0.5348 - val_accuracy: 0.8244\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MUPU26eGPLTi","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}